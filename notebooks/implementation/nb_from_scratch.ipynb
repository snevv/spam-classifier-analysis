{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "from math import log, exp\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Prepare Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training samples: 4457\n",
            "Test samples: 1115\n",
            "\n",
            "Label distribution:\n",
            "label\n",
            "ham     3859\n",
            "spam     598\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Going on nothing great.bye</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>I wont. So wat's wit the guys</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok k..sry i knw 2 siva..tats y i askd..</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>Where are you ? What do you do ? How can you s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Have you not finished work yet or something?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label                                            message\n",
              "0   ham                         Going on nothing great.bye\n",
              "1   ham                      I wont. So wat's wit the guys\n",
              "2   ham            Ok k..sry i knw 2 siva..tats y i askd..\n",
              "3   ham  Where are you ? What do you do ? How can you s...\n",
              "4   ham       Have you not finished work yet or something?"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load data\n",
        "train_df = pd.read_csv('../../data/processed/train.csv')\n",
        "test_df = pd.read_csv('../../data/processed/test.csv')\n",
        "\n",
        "print(f\"Training samples: {len(train_df)}\")\n",
        "print(f\"Test samples: {len(test_df)}\")\n",
        "print(f\"\\nLabel distribution:\")\n",
        "print(train_df['label'].value_counts())\n",
        "\n",
        "train_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Text Preprocessing (Simple Tokenization)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    Simple tokenization: lowercase and split on non-word characters.\n",
        "    \"\"\"\n",
        "    # Convert to lowercase\n",
        "    text = str(text).lower()\n",
        "    # Extract words (alphanumeric sequences)\n",
        "    words = re.findall(r'\\b\\w+\\b', text)\n",
        "    return words\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Naive Bayes Classifier Implementation\n",
        "\n",
        "### Key Components:\n",
        "1. **Prior probabilities**: P(spam) and P(ham)\n",
        "2. **Word probabilities**: P(word | spam) and P(word | ham)\n",
        "3. **Laplace smoothing**: Prevents zero probabilities\n",
        "4. **Log probabilities**: Avoids numerical underflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NaiveBayesClassifier class defined!\n"
          ]
        }
      ],
      "source": [
        "class NaiveBayesClassifier:\n",
        "    \"\"\"\n",
        "    Simplified Naive Bayes classifier for text classification.\n",
        "    \n",
        "    Assumptions:\n",
        "    - Words are conditionally independent given the class (naive assumption)\n",
        "    - Uses multinomial model (counts word occurrences)\n",
        "    - Laplace smoothing to handle unseen words\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, alpha=1.0):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "        -----------\n",
        "        alpha : float\n",
        "            Smoothing parameter (Laplace smoothing). \n",
        "            alpha=1.0 means add-one smoothing.\n",
        "        \"\"\"\n",
        "        self.alpha = alpha\n",
        "        self.vocab = set()\n",
        "        self.word_counts_spam = defaultdict(int)\n",
        "        self.word_counts_ham = defaultdict(int)\n",
        "        self.total_words_spam = 0\n",
        "        self.total_words_ham = 0\n",
        "        self.prior_spam = 0.0\n",
        "        self.prior_ham = 0.0\n",
        "        self.vocab_size = 0\n",
        "        \n",
        "    def fit(self, messages, labels):\n",
        "        \"\"\"\n",
        "        Train the classifier.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        messages : list or pd.Series\n",
        "            Training messages\n",
        "        labels : list or pd.Series\n",
        "            Training labels ('spam' or 'ham')\n",
        "        \"\"\"\n",
        "        print(\"Training Naive Bayes classifier...\")\n",
        "        \n",
        "        # Build vocabulary and count words\n",
        "        for message, label in zip(messages, labels):\n",
        "            tokens = tokenize(message)\n",
        "            self.vocab.update(tokens)\n",
        "            \n",
        "            if label == 'spam':\n",
        "                for token in tokens:\n",
        "                    self.word_counts_spam[token] += 1\n",
        "                    self.total_words_spam += 1\n",
        "            else:  # ham\n",
        "                for token in tokens:\n",
        "                    self.word_counts_ham[token] += 1\n",
        "                    self.total_words_ham += 1\n",
        "        \n",
        "        self.vocab_size = len(self.vocab)\n",
        "        \n",
        "        # Calculate prior probabilities\n",
        "        label_counts = Counter(labels)\n",
        "        total_samples = len(labels)\n",
        "        self.prior_spam = label_counts['spam'] / total_samples\n",
        "        self.prior_ham = label_counts['ham'] / total_samples\n",
        "        \n",
        "        print(f\"Vocabulary size: {self.vocab_size}\")\n",
        "        print(f\"Prior P(spam) = {self.prior_spam:.4f}\")\n",
        "        print(f\"Prior P(ham) = {self.prior_ham:.4f}\")\n",
        "        print(f\"Total words in spam: {self.total_words_spam}\")\n",
        "        print(f\"Total words in ham: {self.total_words_ham}\")\n",
        "        \n",
        "    def get_word_probability(self, word, class_label):\n",
        "        \"\"\"\n",
        "        Calculate P(word | class) with Laplace smoothing.\n",
        "        \n",
        "        Formula: P(word | class) = (count(word, class) + alpha) / (total_words_in_class + alpha * vocab_size)\n",
        "        \"\"\"\n",
        "        if class_label == 'spam':\n",
        "            count = self.word_counts_spam[word]\n",
        "            total = self.total_words_spam\n",
        "        else:  # ham\n",
        "            count = self.word_counts_ham[word]\n",
        "            total = self.total_words_ham\n",
        "        \n",
        "        # Laplace smoothing\n",
        "        numerator = count + self.alpha\n",
        "        denominator = total + self.alpha * self.vocab_size\n",
        "        \n",
        "        return numerator / denominator\n",
        "    \n",
        "    def predict_log_probability(self, message):\n",
        "        \"\"\"\n",
        "        Calculate log P(class | message) for both classes.\n",
        "        \n",
        "        Uses log probabilities to avoid numerical underflow.\n",
        "        \n",
        "        Formula: log P(class | message) = log P(class) + sum(log P(word | class))\n",
        "        \"\"\"\n",
        "        tokens = tokenize(message)\n",
        "        \n",
        "        # Start with log prior\n",
        "        log_prob_spam = log(self.prior_spam)\n",
        "        log_prob_ham = log(self.prior_ham)\n",
        "        \n",
        "        # Add log probabilities for each word\n",
        "        for word in tokens:\n",
        "            if word in self.vocab:  # Only consider words in vocabulary\n",
        "                log_prob_spam += log(self.get_word_probability(word, 'spam'))\n",
        "                log_prob_ham += log(self.get_word_probability(word, 'ham'))\n",
        "        \n",
        "        return log_prob_spam, log_prob_ham\n",
        "    \n",
        "    def predict(self, message):\n",
        "        \"\"\"\n",
        "        Predict class for a message.\n",
        "        \n",
        "        Returns:\n",
        "        --------\n",
        "        str : 'spam' or 'ham'\n",
        "        \"\"\"\n",
        "        log_prob_spam, log_prob_ham = self.predict_log_probability(message)\n",
        "        \n",
        "        # Class with higher log probability wins\n",
        "        return 'spam' if log_prob_spam > log_prob_ham else 'ham'\n",
        "    \n",
        "    def predict_proba(self, message):\n",
        "        \"\"\"\n",
        "        Predict class probabilities.\n",
        "        \n",
        "        Returns:\n",
        "        --------\n",
        "        dict : {'spam': probability, 'ham': probability}\n",
        "        \"\"\"\n",
        "        log_prob_spam, log_prob_ham = self.predict_log_probability(message)\n",
        "        \n",
        "        # Convert log probabilities to probabilities (using log-sum-exp trick)\n",
        "        # To avoid underflow, subtract the maximum\n",
        "        max_log_prob = max(log_prob_spam, log_prob_ham)\n",
        "        prob_spam = exp(log_prob_spam - max_log_prob)\n",
        "        prob_ham = exp(log_prob_ham - max_log_prob)\n",
        "        \n",
        "        # Normalize\n",
        "        total = prob_spam + prob_ham\n",
        "        prob_spam /= total\n",
        "        prob_ham /= total\n",
        "        \n",
        "        return {'spam': prob_spam, 'ham': prob_ham}\n",
        "    \n",
        "    def get_word_contributions(self, message):\n",
        "        \"\"\"\n",
        "        Get contribution of each word to the classification decision.\n",
        "        Useful for interpretability!\n",
        "        \n",
        "        Returns:\n",
        "        --------\n",
        "        list : List of (word, spam_log_prob, ham_log_prob, contribution) tuples\n",
        "        \"\"\"\n",
        "        tokens = tokenize(message)\n",
        "        contributions = []\n",
        "        \n",
        "        log_prob_spam = log(self.prior_spam)\n",
        "        log_prob_ham = log(self.prior_ham)\n",
        "        \n",
        "        for word in tokens:\n",
        "            if word in self.vocab:\n",
        "                word_log_prob_spam = log(self.get_word_probability(word, 'spam'))\n",
        "                word_log_prob_ham = log(self.get_word_probability(word, 'ham'))\n",
        "                \n",
        "                # Contribution = difference in log probabilities\n",
        "                contribution = word_log_prob_spam - word_log_prob_ham\n",
        "                \n",
        "                contributions.append({\n",
        "                    'word': word,\n",
        "                    'spam_log_prob': word_log_prob_spam,\n",
        "                    'ham_log_prob': word_log_prob_ham,\n",
        "                    'contribution': contribution\n",
        "                })\n",
        "                \n",
        "                log_prob_spam += word_log_prob_spam\n",
        "                log_prob_ham += word_log_prob_ham\n",
        "        \n",
        "        return contributions\n",
        "\n",
        "print(\"NaiveBayesClassifier class defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train the Classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Naive Bayes classifier...\n",
            "Vocabulary size: 7742\n",
            "Prior P(spam) = 0.1342\n",
            "Prior P(ham) = 0.8658\n",
            "Total words in spam: 15480\n",
            "Total words in ham: 56439\n"
          ]
        }
      ],
      "source": [
        "# Train classifier\n",
        "nb = NaiveBayesClassifier(alpha=1.0)  # Laplace smoothing\n",
        "nb.fit(train_df['message'], train_df['label'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Evaluate on Test Set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9848\n",
            "\n",
            "Confusion Matrix:\n",
            "                 Predicted\n",
            "              Ham    Spam\n",
            "Actual Ham    961      5\n",
            "       Spam    12    137\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         Ham       0.99      0.99      0.99       966\n",
            "        Spam       0.96      0.92      0.94       149\n",
            "\n",
            "    accuracy                           0.98      1115\n",
            "   macro avg       0.98      0.96      0.97      1115\n",
            "weighted avg       0.98      0.98      0.98      1115\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Make predictions\n",
        "predictions = []\n",
        "probabilities = []\n",
        "\n",
        "for message in test_df['message']:\n",
        "    pred = nb.predict(message)\n",
        "    proba = nb.predict_proba(message)\n",
        "    predictions.append(pred)\n",
        "    probabilities.append(proba)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = sum(p == t for p, t in zip(predictions, test_df['label'])) / len(test_df)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Confusion matrix\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "cm = confusion_matrix(test_df['label'], predictions, labels=['ham', 'spam'])\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(\"                 Predicted\")\n",
        "print(\"              Ham    Spam\")\n",
        "print(f\"Actual Ham   {cm[0,0]:4d}   {cm[0,1]:4d}\")\n",
        "print(f\"       Spam  {cm[1,0]:4d}   {cm[1,1]:4d}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(test_df['label'], predictions, target_names=['Ham', 'Spam']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Most Spam-Indicative and Ham-Indicative Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 20 Spam-Indicative Words:\n",
            "--------------------------------------------------------------------------------\n",
            "claim                | P(spam)=0.004091 | P(ham)=0.000016 | Ratio=262.56\n",
            "prize                | P(spam)=0.002928 | P(ham)=0.000016 | Ratio=187.94\n",
            "å                    | P(spam)=0.010249 | P(ham)=0.000062 | Ratio=164.45\n",
            "150p                 | P(spam)=0.002368 | P(ham)=0.000016 | Ratio=152.01\n",
            "tone                 | P(spam)=0.002024 | P(ham)=0.000016 | Ratio=129.90\n",
            "18                   | P(spam)=0.001809 | P(ham)=0.000016 | Ratio=116.08\n",
            "cs                   | P(spam)=0.001723 | P(ham)=0.000016 | Ratio=110.55\n",
            "guaranteed           | P(spam)=0.001636 | P(ham)=0.000016 | Ratio=105.02\n",
            "500                  | P(spam)=0.001636 | P(ham)=0.000016 | Ratio=105.02\n",
            "1000                 | P(spam)=0.001464 | P(ham)=0.000016 | Ratio=93.97\n",
            "uk                   | P(spam)=0.002756 | P(ham)=0.000031 | Ratio=88.44\n",
            "150ppm               | P(spam)=0.001249 | P(ham)=0.000016 | Ratio=80.15\n",
            "awarded              | P(spam)=0.001206 | P(ham)=0.000016 | Ratio=77.39\n",
            "www                  | P(spam)=0.003531 | P(ham)=0.000047 | Ratio=75.54\n",
            "collection           | P(spam)=0.001120 | P(ham)=0.000016 | Ratio=71.86\n",
            "ringtone             | P(spam)=0.001077 | P(ham)=0.000016 | Ratio=69.10\n",
            "award                | P(spam)=0.001077 | P(ham)=0.000016 | Ratio=69.10\n",
            "rate                 | P(spam)=0.001077 | P(ham)=0.000016 | Ratio=69.10\n",
            "5000                 | P(spam)=0.000990 | P(ham)=0.000016 | Ratio=63.57\n",
            "mob                  | P(spam)=0.000947 | P(ham)=0.000016 | Ratio=60.80\n",
            "\n",
            "================================================================================\n",
            "Top 20 Ham-Indicative Words:\n",
            "--------------------------------------------------------------------------------\n",
            "gt                   | P(spam)=0.000043 | P(ham)=0.004067 | Ratio=0.01\n",
            "lt                   | P(spam)=0.000043 | P(ham)=0.004004 | Ratio=0.01\n",
            "he                   | P(spam)=0.000043 | P(ham)=0.002509 | Ratio=0.02\n",
            "da                   | P(spam)=0.000043 | P(ham)=0.002026 | Ratio=0.02\n",
            "lor                  | P(spam)=0.000043 | P(ham)=0.002026 | Ratio=0.02\n",
            "she                  | P(spam)=0.000043 | P(ham)=0.001963 | Ratio=0.02\n",
            "later                | P(spam)=0.000043 | P(ham)=0.001823 | Ratio=0.02\n",
            "ì_                   | P(spam)=0.000043 | P(ham)=0.001433 | Ratio=0.03\n",
            "ll                   | P(spam)=0.000129 | P(ham)=0.003490 | Ratio=0.04\n",
            "doing                | P(spam)=0.000043 | P(ham)=0.001122 | Ratio=0.04\n",
            "say                  | P(spam)=0.000043 | P(ham)=0.001091 | Ratio=0.04\n",
            "really               | P(spam)=0.000043 | P(ham)=0.001075 | Ratio=0.04\n",
            "ask                  | P(spam)=0.000043 | P(ham)=0.001075 | Ratio=0.04\n",
            "home                 | P(spam)=0.000086 | P(ham)=0.002135 | Ratio=0.04\n",
            "amp                  | P(spam)=0.000043 | P(ham)=0.001060 | Ratio=0.04\n",
            "anything             | P(spam)=0.000043 | P(ham)=0.001060 | Ratio=0.04\n",
            "my                   | P(spam)=0.000388 | P(ham)=0.009255 | Ratio=0.04\n",
            "morning              | P(spam)=0.000043 | P(ham)=0.000982 | Ratio=0.04\n",
            "something            | P(spam)=0.000043 | P(ham)=0.000950 | Ratio=0.05\n",
            "said                 | P(spam)=0.000043 | P(ham)=0.000950 | Ratio=0.05\n"
          ]
        }
      ],
      "source": [
        "# Calculate word probabilities for all words in vocabulary\n",
        "word_scores = []\n",
        "\n",
        "for word in nb.vocab:\n",
        "    spam_prob = nb.get_word_probability(word, 'spam')\n",
        "    ham_prob = nb.get_word_probability(word, 'ham')\n",
        "    \n",
        "    # Ratio: how much more likely in spam vs ham\n",
        "    ratio = spam_prob / ham_prob if ham_prob > 0 else float('inf')\n",
        "    \n",
        "    word_scores.append({\n",
        "        'word': word,\n",
        "        'spam_prob': spam_prob,\n",
        "        'ham_prob': ham_prob,\n",
        "        'ratio': ratio,\n",
        "        'log_ratio': log(ratio) if ratio != float('inf') else 100\n",
        "    })\n",
        "\n",
        "word_scores_df = pd.DataFrame(word_scores)\n",
        "\n",
        "# Top spam words (highest spam/ham ratio)\n",
        "print(\"Top 20 Spam-Indicative Words:\")\n",
        "print(\"-\" * 80)\n",
        "top_spam = word_scores_df.nlargest(20, 'ratio')\n",
        "for idx, row in top_spam.iterrows():\n",
        "    print(f\"{row['word']:20s} | P(spam)={row['spam_prob']:.6f} | P(ham)={row['ham_prob']:.6f} | Ratio={row['ratio']:.2f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "\n",
        "# Top ham words (lowest spam/ham ratio)\n",
        "print(\"Top 20 Ham-Indicative Words:\")\n",
        "print(\"-\" * 80)\n",
        "top_ham = word_scores_df.nsmallest(20, 'ratio')\n",
        "for idx, row in top_ham.iterrows():\n",
        "    print(f\"{row['word']:20s} | P(spam)={row['spam_prob']:.6f} | P(ham)={row['ham_prob']:.6f} | Ratio={row['ratio']:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Key Insights\n",
        "\n",
        "### What This Implementation Shows:\n",
        "\n",
        "1. **Probabilistic Reasoning**: We can see exactly how each word contributes to the final decision\n",
        "2. **Transparency**: Every prediction can be explained by showing word probabilities\n",
        "3. **Interpretability**: Unlike neural networks, we can inspect and understand every step\n",
        "4. **Mathematical Foundation**: Based on Bayes' theorem and conditional independence assumption\n",
        "\n",
        "### Limitations:\n",
        "\n",
        "1. **Naive Assumption**: Assumes words are independent (not true in reality)\n",
        "2. **No Feature Engineering**: Simple tokenization, no TF-IDF weighting\n",
        "3. **Performance**: Not optimized like scikit-learn's implementation\n",
        "4. **Vocabulary**: Limited to words seen in training data\n",
        "\n",
        "### Why This Matters for Your Project:\n",
        "\n",
        "- **Interpretability**: You can show exactly why a message was classified as spam/ham\n",
        "- **Human-like Reasoning**: The probabilistic approach mirrors how humans might reason about spam\n",
        "- **Contrast with Neural Networks**: Highlights the trade-off between interpretability and flexibility\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
